# DPQ 1-13-2016
**Suppose a program you wrote to solve yesterday's problem could analyze a set of one million data value in one second. Estimate how long it should be expected to take to analyze ten million values.
Be sure to justify your estimate; and if it seems to come to more than a minute, try to come up with a better algorithm.**

Based on the algorithm's operations performed (accounting for each of the loops in the algorithm), we can estimate that the algorithm will run on the order of O(n) or linear time (since the input data is significantly greater than the 200 operations from the sorting done on the count variable, which averages around O(n log n)). Therefore, if we can analyze an array of 1 million data values in 1 second, we therefore can analyze 10 million data values in approximately 10 seconds.

> good analysis. It is worth noting that your sorting operation is only applied to at most 100 values, so ends up playing a minor role as far as using time (2.0/1.0)
